{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7wk6aBaqqGN"
      },
      "outputs": [],
      "source": [
        "# importa libraries and data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X = np.genfromtxt(\"X.dat\")  # Features\n",
        "y = np.genfromtxt(\"y.dat\")  # Labels (+1, -1)\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# introduce loss function\n",
        "def loss(w, x, y):\n",
        "  error =  jnp.log( 1 + jnp.exp(-jnp.dot(w,x)*y[0]))\n",
        "  return jnp.mean(error)"
      ],
      "metadata": {
        "id": "tQnI3QgVtrkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 2.1 -> SGD with constant learning rate\n",
        "loss_jit = jax.jit(loss)\n",
        "grad_jit = jax.jit(jax.grad(loss, argnums = 0))\n",
        "\n",
        "epochs = 5000\n",
        "learning_rate = 0.5\n",
        "batch_size = 1\n",
        "n_samples, n_features = X.shape\n",
        "history = list()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  idxs = np.random.choice(n_samples, batch_size)\n",
        "  grad = grad_jit(w, X[idx].reshape(-1), y[idx])\n",
        "  w -= learning_rate * grad\n",
        "  history.append(loss_jit(w, X.T, y))\n",
        "\n",
        "print(loss_jit(w, X.T, y))"
      ],
      "metadata": {
        "id": "hRJVmtJfsCA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE 2.2 -> SGD with non constant learning rate\n",
        "loss_jit = jax.jit(loss)\n",
        "grad_jit = jax.jit(jax.grad(loss, argnums = 0))\n",
        "\n",
        "epochs = 5000\n",
        "learning_rate_0 = 0.05\n",
        "batch_size = 1\n",
        "n_samples, n_features = X.shape\n",
        "history = list()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  learning_rate = learning_rate0/(1+epoch(100))\n",
        "  idxs = np.random.choice(n_samples, batch_size)\n",
        "  grad = grad_jit(w, X[idx].reshape(-1), y[idx])\n",
        "  w -= learning_rate * grad\n",
        "  history.append(loss_jit(w, X.T, y))\n",
        "\n",
        "print(loss_jit(w, X.T, y))"
      ],
      "metadata": {
        "id": "cOwrX7khsEko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare history of loss function for the two methods\n",
        "plt.loglog(loss_history_dlr, label=\"DLR\")\n",
        "plt.loglog(loss_history, label=\"SGD\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "ZEyJt4whs5f4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}